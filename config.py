# Updated HuggingFace Configuration - September 2025
# Based on latest model availability and rate limits

import os
import streamlit as st
from typing import List, Dict, Any

# API Configuration - HuggingFace Only
HUGGINGFACE_API_URL = "https://api-inference.huggingface.co"

def get_huggingface_token():
    """Get HuggingFace API token"""
    token = None
    try:
        token = st.secrets["HUGGINGFACE_TOKEN"]
        if token and len(token) > 10:
            return token
    except Exception:
        pass
    
    token = os.getenv("HUGGINGFACE_TOKEN", "")
    if token and len(token) > 10:
        return token
    
    return ""

HUGGINGFACE_TOKEN = get_huggingface_token()

# ‚úÖ CONFIRMED AVAILABLE MODELS - September 2025
# Based on research of current HF Inference API availability
HUGGINGFACE_MODELS = {
    # Primary models (confirmed available on free tier)
    "mistral_7b_v02": "mistralai/Mistral-7B-Instruct-v0.2",      # Proven stable
    "mistral_7b_v03": "mistralai/Mistral-7B-Instruct-v0.3",      # Latest with extended vocab
    
    # Reliable Google models (always available)
    "flan_t5_xl": "google/flan-t5-xl",                           # Excellent for structured tasks
    "flan_ul2": "google/flan-ul2",                               # More capable reasoning
    
    # Microsoft models (generally available)
    "dialo_gpt": "microsoft/DialoGPT-medium",                    # Good for conversations
    
    # Always available fallbacks
    "gpt2_medium": "gpt2-medium",                                # Reliable baseline
    "distilbert": "distilbert-base-uncased",                     # Lightweight
    
    # Meta models (check availability)
    "llama2_7b_chat": "meta-llama/Llama-2-7b-chat-hf",         # If accessible
    "code_llama_7b": "codellama/CodeLlama-7b-Instruct-hf",      # For tech resumes
}

# MODEL SELECTION - Prioritized by reliability and performance
# Primary choice: Mistral 7B v0.3 (latest, best performance)
# Fallback: Mistral 7B v0.2 (proven stable)
# Secondary: FLAN-T5 XL (reliable for structured tasks)

DATA_EXTRACTION_MODEL = HUGGINGFACE_MODELS["distilbert"]     # Latest Mistral
RELEVANCE_SCORING_MODEL = HUGGINGFACE_MODELS["distilbert"]   # Latest Mistral  
CHATBOT_MODEL = HUGGINGFACE_MODELS["distilbert"]            # Latest Mistral
MAIN_MODEL = HUGGINGFACE_MODELS["distilbert"]               # Latest Mistral

# FALLBACK CHAIN - If primary models fail
FALLBACK_MODELS = [
    HUGGINGFACE_MODELS["mistral_7b_v02"],  # Stable Mistral
    HUGGINGFACE_MODELS["flan_t5_xl"],      # Google reliable
    HUGGINGFACE_MODELS["flan_ul2"],        # Google advanced
    HUGGINGFACE_MODELS["gpt2_medium"]      # Always works
]

# Updated rate limiting settings based on September 2025 limits
HUGGINGFACE_SETTINGS = {
    "data_extraction": {
        "max_length": 800,           # Conservative for free tier (300 req/hour)
        "temperature": 0.1,          # Focused for data extraction
        "do_sample": True,
        "top_p": 0.8,
        "timeout": 90,               # Allow for model loading
        "wait_for_model": True,      # Critical for free tier
        "max_retries": 3,
        "retry_delay": 15            # Longer delays for rate limits
    },
    "relevance_scoring": {
        "max_length": 400,           # Shorter for scoring
        "temperature": 0.2,          # Slightly more analytical
        "do_sample": True,
        "top_p": 0.9,
        "timeout": 75,
        "wait_for_model": True,
        "max_retries": 3,
        "retry_delay": 12
    },
    "chatbot": {
        "max_length": 250,           # Conversational length
        "temperature": 0.4,          # More creative for chat
        "do_sample": True,
        "top_p": 0.95,
        "timeout": 60,
        "wait_for_model": True,
        "max_retries": 2,
        "retry_delay": 10
    }
}

# Rate limiting configuration (updated for September 2025)
RATE_LIMITS = {
    "free_tier": {
        "requests_per_hour": 300,
        "burst_limit": 10,           # Conservative burst
        "delay_between_requests": 15, # Seconds between requests
        "model_loading_timeout": 120  # Allow 2 minutes for model loading
    },
    "pro_tier": {
        "requests_per_hour": 1000,
        "burst_limit": 30,
        "delay_between_requests": 4,
        "model_loading_timeout": 60
    }
}

# Scoring configuration
SCORING_WEIGHTS = {
    "relevance_score": 0.65,     # Primary: LLM relevance scoring
    "experience_match": 0.25,    # Experience alignment 
    "skills_match": 0.10         # Skills extraction and matching
}

# File processing settings
UPLOAD_FOLDER = "./data/uploads"
CACHE_PATH = "./data/hf_cache"
MAX_FILE_SIZE = 15 * 1024 * 1024  # 15MB

# Thresholds
MIN_SCORE_THRESHOLD = 0.3
TOP_CANDIDATES = 15

# Job templates (keep existing)
JOB_TEMPLATES = {
    "Data Scientist": """
    Data Scientist position requiring Python, SQL, machine learning experience with 
    TensorFlow/PyTorch, statistical analysis skills, 3+ years experience, 
    Master's degree preferred, cloud platforms knowledge, strong problem-solving abilities.
    """,
    
    "Software Engineer": """
    Software Engineer role requiring programming skills in Python/JavaScript/Java,
    web development frameworks, database knowledge, 2+ years experience,
    Computer Science degree, version control (Git), agile methodologies experience.
    """,
    
    "Marketing Manager": """
    Marketing Manager position requiring 5+ years marketing experience, team leadership,
    digital marketing expertise (SEO, SEM, social media), analytics tools experience,
    campaign management, Bachelor's degree in Marketing/Business, excellent communication skills.
    """,
    
    "Product Manager": """
    Product Manager role requiring 3+ years product management experience,
    product lifecycle management, analytical skills, Agile/Scrum methodology,
    technical background, market research skills, stakeholder management abilities.
    """
}

# Updated prompts optimized for Mistral 7B v0.3
HUGGINGFACE_PROMPTS = {
    "data_extraction": """<s>[INST] Extract structured information from this resume.

Resume Text:
{resume_text}

Provide ONLY a JSON response with this exact structure:
{{
    "name": "full name",
    "email": "email address", 
    "phone": "phone number",
    "experience_years": number,
    "current_role": "latest job title",
    "skills": ["skill1", "skill2", "skill3"],
    "education": "highest degree",
    "companies": ["company1", "company2"],
    "summary": "brief professional summary"
}}

Respond with valid JSON only. [/INST]""",

    "relevance_scoring": """<s>[INST] Score this candidate's fit for the job from 0-100.

Job Requirements:
{job_description}

Candidate Profile:
{candidate_data}

Evaluate: technical skills (30%), experience (35%), education (15%), growth (10%), cultural fit (10%).

Format: Score: XX
Reason: Brief explanation (max 100 words)

[/INST]""",

    "detailed_analysis": """<s>[INST] Analyze this candidate for the role.

Job: {job_title}
Requirements: {job_description}

Candidate: {candidate_data}

Provide:
1. Key Strengths (3 points)
2. Concerns (2 points)  
3. Interview Focus (2 areas)
4. Recommendation (Hire/Consider/Pass)

Keep under 250 words. [/INST]""",

    "chatbot": """<s>[INST] You are an HR assistant. Answer this hiring question professionally.

Context: {context}

Question: {question}

Provide helpful advice in 2-3 sentences. [/INST]"""
}

# Model availability checker
def check_model_availability():
    """Check which models are currently available"""
    available_models = []
    unavailable_models = []
    
    # This would need to be implemented with actual API calls
    # For now, return based on known availability
    
    # Confirmed available (September 2025)
    available_models = [
        "mistralai/Mistral-7B-Instruct-v0.3",
        "mistralai/Mistral-7B-Instruct-v0.2", 
        "google/flan-t5-xl",
        "google/flan-ul2",
        "gpt2-medium"
    ]
    
    return {
        "available": available_models,
        "unavailable": unavailable_models,
        "primary_model": DATA_EXTRACTION_MODEL,
        "fallback_chain": FALLBACK_MODELS,
        "last_checked": "September 2025"
    }

# Cost estimation (updated for September 2025)
def get_cost_estimates(num_resumes: int) -> Dict[str, Any]:
    """Get updated cost estimates for September 2025"""
    
    # Free tier: 300 requests/hour, $0.10/month credits
    calls_per_resume = 2.7  # Average calls per resume
    total_calls = calls_per_resume * num_resumes
    
    # Time estimation (with rate limits)
    if total_calls <= 300:  # Within hourly limit
        processing_time_minutes = num_resumes * 2  # ~2 min per resume
    else:
        # Will hit rate limits
        hours_needed = total_calls / 300
        processing_time_minutes = hours_needed * 60
    
    return {
        "num_resumes": num_resumes,
        "total_calls": int(total_calls),
        "free_tier": {
            "cost": "$0.00",
            "monthly_limit": "Uses $0.10 monthly credits",
            "estimated_time_minutes": round(processing_time_minutes, 1),
            "rate_limit_impact": "High" if total_calls > 300 else "None"
        },
        "pro_tier": {
            "cost": "$9/month + usage",
            "monthly_credits": "$2 included",
            "estimated_time_minutes": round(num_resumes * 1.5, 1),
            "rate_limit_impact": "Minimal"
        },
        "recommendations": [
            "Free tier: Process in batches of 10-15 resumes",
            "Pro tier: Unlimited processing for $9/month",
            "Rate limits reset every hour on free tier",
            "Consider Pro for >20 resumes at once"
        ]
    }

# Directory creation
def create_directories():
    """Create necessary directories"""
    directories = [
        UPLOAD_FOLDER,
        CACHE_PATH,
        "./data/temp_uploads",
        "./logs"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

# Validation function
def validate_setup():
    """Validate HuggingFace setup with September 2025 info"""
    issues = []
    
    if not HUGGINGFACE_TOKEN:
        issues.append("‚ùå Missing HuggingFace API token")
        issues.append("üîß Add HUGGINGFACE_TOKEN to .streamlit/secrets.toml")
        issues.append("üìù Get free token from https://huggingface.co/settings/tokens")
        issues.append("üí° Free tier: 300 requests/hour, $0.10/month credits")
    elif len(HUGGINGFACE_TOKEN) < 20:
        issues.append("‚ùå Invalid HuggingFace token format")
    else:
        issues.append("‚úÖ HuggingFace API configured")
        issues.append("‚úÖ Using Mistral 7B v0.3 (latest)")
        issues.append("‚úÖ FREE tier: 300 requests/hour")
        issues.append("üí° Upgrade to Pro ($9/month) for 1000 requests/hour")
        issues.append("üéâ Zero base cost - only rate limits!")
    
    return issues

# Updated rate limit info
def get_rate_limit_info():
    """Get current rate limiting information"""
    return {
        "free_tier": {
            "requests_per_hour": 300,
            "monthly_credits": "$0.10",
            "model_size_limit": "10GB (popular models excepted)",
            "burst_capability": "Limited"
        },
        "pro_tier": {
            "requests_per_hour": 1000,
            "monthly_cost": "$9",
            "monthly_credits": "$2 included",
            "additional_benefits": ["Priority access", "Premium models", "Faster processing"]
        },
        "updated": "September 2025",
        "source": "HuggingFace Documentation & Community Reports"
    }
